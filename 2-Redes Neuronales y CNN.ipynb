{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDES NEURONALES #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INTRODUCCION ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sesión vamos a trabajar sobre redes neuronales. Primero veremos la teoria de una red neuronal y como construir y entrenar una red neuronal desde cero para luego pasar a estudiar redes de convolución. \n",
    "\n",
    "Los temas de esta sesión son los siguientes:\n",
    "1. **Repasar la teoría de redes neuronales:**\n",
    "    1. _Capas de una red y Función de activación_. \n",
    "    2. _Función objetivo (Loss Function)_.\n",
    "    3. _Backpropagation y optimización_.\n",
    "2. **Construir una red neuronal de una sola capa**\n",
    "3. **Construir una red neuronal de multiples capas**\n",
    "4. **Redes de convolución**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Los ejercicios practicos de esta sección están basados en el curso:**  \n",
    "[Machine Learning Practical (MLP)](http://www.inf.ed.ac.uk/teaching/courses/mlp/index-2018.html) University of Edinburgh\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RED NEURONAL ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 TEORÍA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 PERCEPTRON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/perceptron.png\" style=\"width: 600px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 MULTILAYER PERCEPTRON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/mlp.png\" style=\"width: 600px;\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado un vector $ x $ de entrada a una red neuronal como la de la imagen:\n",
    "- Para cada neurona en la red se realiza un producto escalar de vectores ($w_{1n}^Tx$) al cual se le aplica una función no lineal $f$ (función de activación). El resultado de cada neurona es $f(w_{1n}^Tx)$.\n",
    "- Si se realiza esta operación para cada neurona se puede definir la salidad de la primera capa como $h=f(W_1x)$\n",
    "- La capa de salidad de la red (la vamos a denominar $y$) va a quedar definida por la ecuación $y=f(w_2^Tf(W_1x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 FUNCIONES DE ACTIVACIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _SIGMOID_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/sigmoid.png\" style=\"width: 600px;\" align=\"left\">\n",
    "\n",
    "# $f(x) = \\frac{1}{1 + e^{-x} }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit: Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _RECTIFIED LINEAR UNIT (RELU)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ y = max(0,a) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/relu1.png\" style=\"width: 450px;\" align=\"left\">\n",
    "<img src=\"files/images/relu2.png\" style=\"width: 450px;\" align=\"rigth\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _SOFTMAX_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/images/softmax.png\" style=\"width: 600px;\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 FUNCIONES DE ERROR ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _REGRESIÓN_ ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para un problema de regresion existen varias funciones de error que se pueden utilizar. Una de las funciones más comunes a optimizar es el _Mean Squared Error_ (MSE).\n",
    "\n",
    "El objetivo de un problema de regression es dado unos dataos de entrada $\\left\\lbrace \\boldsymbol{x}^{(n)}\\right\\rbrace_{n=1}^N$ producir valores a la salida de la red $\\left\\lbrace \\boldsymbol{y}^{(n)}\\right\\rbrace_{n=1}^N$ que sean lo más cercanos posibles a  $\\left\\lbrace \\boldsymbol{t}^{(n)}\\right\\rbrace_{n=1}^N$. La medida que se utiliza para medir que tan cercanos son los valores es una decisión de diseño del problema. \n",
    "\n",
    "Una medida de error muy cómun es el cuadrado de la  distancia euclidiana. Esta se calcula haciendo la sumatoria de las diferencias cuadráticas entre las salidas esperadas y la salida de la red. Por otro lado es común multiplicar la salida por $\\frac{1}{2}$ dado que esto genera una expresión más simple a la hora de calcular el gradiente. El error para el ejemplo de entrenamiento número $n^{\\textrm{th}}$ es:\n",
    "\n",
    "$$\n",
    "    E^{(n)} = \\frac{1}{2} \\sum_{k=1}^K \\left\\lbrace \\left( y^{(n)}_k - t^{(n)}_k \\right)^2 \\right\\rbrace.\n",
    "$$\n",
    "\n",
    "El error general se calcula como el promedio del error para todas las muestras de entrenamiento.\n",
    "\n",
    "$$\n",
    "    \\bar{E} = \\frac{1}{N} \\sum_{n=1}^N \\left\\lbrace E^{(n)} \\right\\rbrace. \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 IMPLEMENTACION: RED DE UNA SOLA CAPA (SINGLE LAYER NETWORK)##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para poder optimizar una red correctamente hay que poder calcular los siguientes términos:\n",
    "1. Salida de la red en función de una entrada.\n",
    "2. La función objetivo a optimizar a la salidad de la red.\n",
    "3. El gradiente de la función objectivo  (_Loss Function_)  en función de los parámetros de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 27092016 \n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 _FORWARD PROPAGATION_ ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fprop(inputs, weights, biases):\n",
    "    \"\"\"Forward propagates activations through the layer transformation.\n",
    "\n",
    "    For inputs `x`, outputs `y`, weights `W` and biases `b` the layer\n",
    "    corresponds to `y = W x + b`.\n",
    "\n",
    "    Args:\n",
    "        inputs: Array of layer inputs of shape (batch_size, input_dim).\n",
    "        weights: Array of weight parameters of shape \n",
    "            (output_dim, input_dim).\n",
    "        biases: Array of bias parameters of shape (output_dim, ).\n",
    "\n",
    "    Returns:\n",
    "        outputs: Array of layer outputs of shape (batch_size, output_dim).\n",
    "    \"\"\"\n",
    "    raise NotImplementedError('Delete this raise statement and write your code here instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 _VERIFICAR RESULTADOS_ ###  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[0., -1., 2.], [-6., 3., 1.]])\n",
    "weights = np.array([[2., -3., -1.], [-5., 7., 2.]])\n",
    "biases = np.array([5., -3.])\n",
    "true_outputs = np.array([[6., -6.], [-17., 50.]])\n",
    "\n",
    "if not np.allclose(fprop(inputs, weights, biases), true_outputs):\n",
    "    print('Wrong outputs computed.')\n",
    "else:\n",
    "    print('All outputs correct!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 _FUNCIÓN DE ERROR_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(outputs, targets):\n",
    "    \"\"\"Calculates error function given a batch of outputs and targets.\n",
    "\n",
    "    Args:\n",
    "        outputs: Array of model outputs of shape (batch_size, output_dim).\n",
    "        targets: Array of target outputs of shape (batch_size, output_dim).\n",
    "\n",
    "    Returns:\n",
    "        Scalar error function value.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError('Delete this raise statement and write your code here instead.')\n",
    "    \n",
    "def error_grad(outputs, targets):\n",
    "    \"\"\"Calculates gradient of error function with respect to model outputs.\n",
    "\n",
    "    Args:\n",
    "        outputs: Array of model outputs of shape (batch_size, output_dim).\n",
    "        targets: Array of target outputs of shape (batch_size, output_dim).\n",
    "\n",
    "    Returns:\n",
    "        Gradient of error function with respect to outputs.\n",
    "        This will be an array of shape (batch_size, output_dim).\n",
    "    \"\"\"\n",
    "    raise NotImplementedError('Delete this raise clause and write your code here instead.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 _VERIFICAR RESULTADOS_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.array([[1., 2.], [-1., 0.], [6., -5.], [-1., 1.]])\n",
    "targets = np.array([[0., 1.], [3., -2.], [7., -3.], [1., -2.]])\n",
    "true_error = 5.\n",
    "true_error_grad = np.array([[0.25, 0.25], [-1., 0.5], [-0.25, -0.5], [-0.5, 0.75]])\n",
    "\n",
    "if not error(outputs, targets) == true_error:\n",
    "    print('Error calculated incorrectly.')\n",
    "elif not np.allclose(error_grad(outputs, targets), true_error_grad):\n",
    "    print('Error gradient calculated incorrectly.')\n",
    "else:\n",
    "    print('Error function and gradient computed correctly!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
